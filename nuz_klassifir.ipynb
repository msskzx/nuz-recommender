{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuz Klassifir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "direk_folders = ['culture', 'diverse', 'economy', 'internationalNews', 'localNews', 'politic', 'society', 'sport', 'technology' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "max_samples = 100\n",
    "\n",
    "for folder_name in direk_folders:\n",
    "    direk = 'train/{}/'.format(folder_name)\n",
    "    current_files_names = os.listdir(direk)\n",
    "    \n",
    "    i = 0\n",
    "    for file_name in current_files_names:\n",
    "        \n",
    "        # stop at max number of samples to avoid bias for some classes\n",
    "        i += 1\n",
    "        if i == max_samples:\n",
    "            break\n",
    "            \n",
    "        # read file\n",
    "        file_reader = open(direk + file_name, \"r\", encoding=\"utf-8\")\n",
    "        file = file_reader.read()\n",
    "\n",
    "        # lowercase\n",
    "        file = file.lower()\n",
    "        file = file.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "        # extract title text\n",
    "        title = re.search(r'<title>(.*?)</(.?)title>', file)\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        clean_title = re.sub(cleanr, '', title.group(0))\n",
    "    \n",
    "        # extract abstract text\n",
    "        abstr = re.search(r'<abstract>(.*?)</(.?)abstract>', file)\n",
    "        clean_abstr = re.sub(cleanr, '', abstr.group(0))\n",
    "\n",
    "        # extract text of the main content\n",
    "        text = re.search(r'<text>(.*?)</(.?)text>', file)\n",
    "        clean_text = re.sub(cleanr, '', text.group(0))\n",
    "     \n",
    "        clean_input = clean_title + clean_abstr + clean_text\n",
    "        train.append([clean_input, folder_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL, special characters, and digits removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['مخرج غدوة حي الفيلم يجمع التشويق و المغامرة و الفرجة و الموسيقى استضاف برنامج سينما سينما الأربعاء جانفي في موعد استثنائي أبطال الفيلم التونسي الطويل غدوة حي demain dès l aube و مخرج العمل لطفي عاشور هذه الاستضافة تأتي في إطار انطلاق عرض الفيلم على مستوى الجهات و البداية كانت بقاعة السينما الوحيدة في سوسة و يقول المخرج التونسي لطفي عاشور إن أحداث الفيلم تتمثل في تحقيق قضائي لحادثة وقعت ليلة جانفي و تتواصل أطوارها بعد سنوات من هذا التاريخ بأسلوب فيه الكثير من التشويق و المغامرة و الفرجة و توظيف الموسيقى بشكل مدروس و دقيق وهو ما أثار إعجاب المشاهدين حسب قوله من بين الحضور أي ضا أشرف بن يوسف الذي عب ر عن إعجابه بخوض تجربة التمثيل لأول مر ة رفقة نخبة من الممثلين الذين شاركهم دور البطولة في الفيلم مضيفا أن ه اشتغل على تطوير أداءه قبل خوض هذه التجربة من جهتها بطلة غدوة حي الممثل ة أنيسة داود قالت إن أحداث الفيلم تروي قصة شخصيات أجبرتهم الأحداث التي عاشوها ليلة جانفي على أن ي صبحوا أصدقاء رغم التناقضات مشيرة إلى أن المشاهد سيجد نفسه أمام أداء ممي ز لبعض الوجوه الشابة و سيعيش لحظات طريفة من ناحية و لحظات تراجيدية من ناحية أخرى و شارك في هذا الفيلم عدد من الممثلين من بينهم لطيفة القفصي وعيسى حراث و غازي الزغباني و محمد الداهش و أنيسة داود و د ري ة عاشور وأشرف بن يوسف ', 'culture'], [' و ه فيلم كوميدي جديد في قاعات السينما ابتداء من ديسمبر و ه فيلم كوميدي تونسي جديد لاسمهان لحمر تم العرض ما قبل الأول له يوم ديسمبر بقاعة الكوليزي بتونس العاصمة ومن المنتظر أن يتم عرضه بصفة رسمية في قاعات السينما يوم ديسمبر الحالي و ه فيلم كوميدي تونسي جديد لاسمهان لحمر تم العرض ما قبل الأول له يوم ديسمبر بقاعة الكوليزي بتونس العاصمة ومن المنتظر أن يتم عرضه بصفة رسمية في قاعات السينما يوم ديسمبر الحالي ويجمع هذا الفيلم ثلة من كبار الممثلين على غرار فاطمة بن سعيدان ومريم بن شعبان وسنية بن جمعة وكنزة بن ذياب ونادية بوستة وسناء عز الدين رستم ونعيمة الجاني وجميلة الشيحي وهشام رستم وصلاح مصدق وسامي التميمي ويشارك فيه أيضا مغني الراب أحمد العبيدي كافون ويروي الفيلم قصة عائلة تونسية عادية فالأم زكية هي المسيرة الحقيقية التي تعشق ابنها الأصغر المدلل سليم لدرجة أنها تتناسى بقية الأسرة والأب حامد فضل الانسحاب من الحياة الأسرية والاهتمام بفريقه المفضل الترجي أما باقي أفراد العائلة فأمنيتهم الوحيدة هي زواج سليم وخروجه من حياتهم خاصة الأخت الصغرى أماني التي تقوم بتحضيرات زواجها وهي الوحيدة التي لا تزال تعيش مع والديها ولا تتردد الأم في تأخير زواج ابنتها أماني للاحتفال بعيد ميلاد ابنها المفضل سليم والذي يجب أن يكون احتفالا لم يسبق له مثيل بل تجبر الإخوة على المشاركة في الأعمال التحضيرية بغض النظر عن التكلفة المادية الباهضة للحفل وما سيخلفه من خلافات بين الأبناء ويوم الاحتفال الموعود تقوم أماني بتنفيذ خطة تفاجئ الجميع ويعتبر وه الفيلم الروائي الطويل الأول لاسمهان لحمر التي قامت باخراج فيلمين وثائقيين mon و انت الصوت وفيلمين قصيرين لاشيء و قوس قزح ', 'culture'], ['اختتام المهرجان العربي للإذاعة والتلفزيون تتويج للتلفزة التونسية وإذاعة الديوان أف أم اختتمت ليلة البارحة فعاليات المهرجان العربي للاذاعة و التلفزيون في دورته التي انطلقت يوم أفريل اختتمت ليلة البارحة فعاليات المهرجان العربي للاذاعة و التلفزيون في دورته التي انطلقت يوم أفريل وقررت إدارة المهرجان حجب جوائز المسابقة الاذاعية الرئيسية لهذه الدورة حسب بيان لاتحاد اذاعات الدول العربية وقد تم خلال هذه الدورة تتويج بالنسبة لمسابقة البرامج الوثائقية الخاصة بالقدس العربي الجائزة الاولى القدس قصة صمود من انتاج مؤسسة التلفزة التونسية الجائزة الثانية أسرار القدس من انتاج التلفزة الفلسطينية بالنسبة للبرامج الاذاعية حول القضية الفلسطينية الجائزة الأولى برنامج باب المغاربة الإذاعة الخاصة الديوان أف أم الجائزة الثانية برنامج القدس درة المدائن من انتاج اذاعة زايد للقرأن الكريم الامارات بالنسبة لبرامج الاطفال الجائزة الاولى برنامج كوني من انتاج المؤسسة العمومية للتلفزيون الجزائري الجائزة الثانية برنامج مهنتي من انتاج تلفزيون دولة الكويت ', 'culture']]\n"
     ]
    }
   ],
   "source": [
    "for row in train:\n",
    "    row[0] = re.sub(r'http\\S+', '', row[0])\n",
    "    # remove special characters and digits\n",
    "    row[0] = re.sub(\"(\\\\d|\\\\W)+\",\" \", row[0])\n",
    "print(train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['مخرج', 'غدوة', 'حي', 'الفيلم', 'يجمع', 'التشويق', 'و', 'المغامرة', 'و', 'الفرجة', 'و', 'الموسيقى', 'استضاف', 'برنامج', 'سينما', 'سينما', 'الأربعاء', 'جانفي', 'في', 'موعد', 'استثنائي', 'أبطال', 'الفيلم', 'التونسي', 'الطويل', 'غدوة', 'حي', 'demain', 'dè', 'l', 'aub', 'و', 'مخرج', 'العمل', 'لطفي', 'عاشور', 'هذه', 'الاستضافة', 'تأتي', 'في', 'إطار', 'انطلاق', 'عرض', 'الفيلم', 'على', 'مستوى', 'الجهات', 'و', 'البداية', 'كانت', 'بقاعة', 'السينما', 'الوحيدة', 'في', 'سوسة', 'و', 'يقول', 'المخرج', 'التونسي', 'لطفي', 'عاشور', 'إن', 'أحداث', 'الفيلم', 'تتمثل', 'في', 'تحقيق', 'قضائي', 'لحادثة', 'وقعت', 'ليلة', 'جانفي', 'و', 'تتواصل', 'أطوارها', 'بعد', 'سنوات', 'من', 'هذا', 'التاريخ', 'بأسلوب', 'فيه', 'الكثير', 'من', 'التشويق', 'و', 'المغامرة', 'و', 'الفرجة', 'و', 'توظيف', 'الموسيقى', 'بشكل', 'مدروس', 'و', 'دقيق', 'وهو', 'ما', 'أثار', 'إعجاب', 'المشاهدين', 'حسب', 'قوله', 'من', 'بين', 'الحضور', 'أي', 'ضا', 'أشرف', 'بن', 'يوسف', 'الذي', 'عب', 'ر', 'عن', 'إعجابه', 'بخوض', 'تجربة', 'التمثيل', 'لأول', 'مر', 'ة', 'رفقة', 'نخبة', 'من', 'الممثلين', 'الذين', 'شاركهم', 'دور', 'البطولة', 'في', 'الفيلم', 'مضيفا', 'أن', 'ه', 'اشتغل', 'على', 'تطوير', 'أداءه', 'قبل', 'خوض', 'هذه', 'التجربة', 'من', 'جهتها', 'بطلة', 'غدوة', 'حي', 'الممثل', 'ة', 'أنيسة', 'داود', 'قالت', 'إن', 'أحداث', 'الفيلم', 'تروي', 'قصة', 'شخصيات', 'أجبرتهم', 'الأحداث', 'التي', 'عاشوها', 'ليلة', 'جانفي', 'على', 'أن', 'ي', 'صبحوا', 'أصدقاء', 'رغم', 'التناقضات', 'مشيرة', 'إلى', 'أن', 'المشاهد', 'سيجد', 'نفسه', 'أمام', 'أداء', 'ممي', 'ز', 'لبعض', 'الوجوه', 'الشابة', 'و', 'سيعيش', 'لحظات', 'طريفة', 'من', 'ناحية', 'و', 'لحظات', 'تراجيدية', 'من', 'ناحية', 'أخرى', 'و', 'شارك', 'في', 'هذا', 'الفيلم', 'عدد', 'من', 'الممثلين', 'من', 'بينهم', 'لطيفة', 'القفصي', 'وعيسى', 'حراث', 'و', 'غازي', 'الزغباني', 'و', 'محمد', 'الداهش', 'و', 'أنيسة', 'داود', 'و', 'د', 'ري', 'ة', 'عاشور', 'وأشرف', 'بن', 'يوسف'], 'culture'], [['و', 'ه', 'فيلم', 'كوميدي', 'جديد', 'في', 'قاعات', 'السينما', 'ابتداء', 'من', 'ديسمبر', 'و', 'ه', 'فيلم', 'كوميدي', 'تونسي', 'جديد', 'لاسمهان', 'لحمر', 'تم', 'العرض', 'ما', 'قبل', 'الأول', 'له', 'يوم', 'ديسمبر', 'بقاعة', 'الكوليزي', 'بتونس', 'العاصمة', 'ومن', 'المنتظر', 'أن', 'يتم', 'عرضه', 'بصفة', 'رسمية', 'في', 'قاعات', 'السينما', 'يوم', 'ديسمبر', 'الحالي', 'و', 'ه', 'فيلم', 'كوميدي', 'تونسي', 'جديد', 'لاسمهان', 'لحمر', 'تم', 'العرض', 'ما', 'قبل', 'الأول', 'له', 'يوم', 'ديسمبر', 'بقاعة', 'الكوليزي', 'بتونس', 'العاصمة', 'ومن', 'المنتظر', 'أن', 'يتم', 'عرضه', 'بصفة', 'رسمية', 'في', 'قاعات', 'السينما', 'يوم', 'ديسمبر', 'الحالي', 'ويجمع', 'هذا', 'الفيلم', 'ثلة', 'من', 'كبار', 'الممثلين', 'على', 'غرار', 'فاطمة', 'بن', 'سعيدان', 'ومريم', 'بن', 'شعبان', 'وسنية', 'بن', 'جمعة', 'وكنزة', 'بن', 'ذياب', 'ونادية', 'بوستة', 'وسناء', 'عز', 'الدين', 'رستم', 'ونعيمة', 'الجاني', 'وجميلة', 'الشيحي', 'وهشام', 'رستم', 'وصلاح', 'مصدق', 'وسامي', 'التميمي', 'ويشارك', 'فيه', 'أيضا', 'مغني', 'الراب', 'أحمد', 'العبيدي', 'كافون', 'ويروي', 'الفيلم', 'قصة', 'عائلة', 'تونسية', 'عادية', 'فالأم', 'زكية', 'هي', 'المسيرة', 'الحقيقية', 'التي', 'تعشق', 'ابنها', 'الأصغر', 'المدلل', 'سليم', 'لدرجة', 'أنها', 'تتناسى', 'بقية', 'الأسرة', 'والأب', 'حامد', 'فضل', 'الانسحاب', 'من', 'الحياة', 'الأسرية', 'والاهتمام', 'بفريقه', 'المفضل', 'الترجي', 'أما', 'باقي', 'أفراد', 'العائلة', 'فأمنيتهم', 'الوحيدة', 'هي', 'زواج', 'سليم', 'وخروجه', 'من', 'حياتهم', 'خاصة', 'الأخت', 'الصغرى', 'أماني', 'التي', 'تقوم', 'بتحضيرات', 'زواجها', 'وهي', 'الوحيدة', 'التي', 'لا', 'تزال', 'تعيش', 'مع', 'والديها', 'ولا', 'تتردد', 'الأم', 'في', 'تأخير', 'زواج', 'ابنتها', 'أماني', 'للاحتفال', 'بعيد', 'ميلاد', 'ابنها', 'المفضل', 'سليم', 'والذي', 'يجب', 'أن', 'يكون', 'احتفالا', 'لم', 'يسبق', 'له', 'مثيل', 'بل', 'تجبر', 'الإخوة', 'على', 'المشاركة', 'في', 'الأعمال', 'التحضيرية', 'بغض', 'النظر', 'عن', 'التكلفة', 'المادية', 'الباهضة', 'للحفل', 'وما', 'سيخلفه', 'من', 'خلافات', 'بين', 'الأبناء', 'ويوم', 'الاحتفال', 'الموعود', 'تقوم', 'أماني', 'بتنفيذ', 'خطة', 'تفاجئ', 'الجميع', 'ويعتبر', 'وه', 'الفيلم', 'الروائي', 'الطويل', 'الأول', 'لاسمهان', 'لحمر', 'التي', 'قامت', 'باخراج', 'فيلمين', 'وثائقيين', 'mon', 'و', 'انت', 'الصوت', 'وفيلمين', 'قصيرين', 'لاشيء', 'و', 'قوس', 'قزح'], 'culture'], [['اختتام', 'المهرجان', 'العربي', 'للإذاعة', 'والتلفزيون', 'تتويج', 'للتلفزة', 'التونسية', 'وإذاعة', 'الديوان', 'أف', 'أم', 'اختتمت', 'ليلة', 'البارحة', 'فعاليات', 'المهرجان', 'العربي', 'للاذاعة', 'و', 'التلفزيون', 'في', 'دورته', 'التي', 'انطلقت', 'يوم', 'أفريل', 'اختتمت', 'ليلة', 'البارحة', 'فعاليات', 'المهرجان', 'العربي', 'للاذاعة', 'و', 'التلفزيون', 'في', 'دورته', 'التي', 'انطلقت', 'يوم', 'أفريل', 'وقررت', 'إدارة', 'المهرجان', 'حجب', 'جوائز', 'المسابقة', 'الاذاعية', 'الرئيسية', 'لهذه', 'الدورة', 'حسب', 'بيان', 'لاتحاد', 'اذاعات', 'الدول', 'العربية', 'وقد', 'تم', 'خلال', 'هذه', 'الدورة', 'تتويج', 'بالنسبة', 'لمسابقة', 'البرامج', 'الوثائقية', 'الخاصة', 'بالقدس', 'العربي', 'الجائزة', 'الاولى', 'القدس', 'قصة', 'صمود', 'من', 'انتاج', 'مؤسسة', 'التلفزة', 'التونسية', 'الجائزة', 'الثانية', 'أسرار', 'القدس', 'من', 'انتاج', 'التلفزة', 'الفلسطينية', 'بالنسبة', 'للبرامج', 'الاذاعية', 'حول', 'القضية', 'الفلسطينية', 'الجائزة', 'الأولى', 'برنامج', 'باب', 'المغاربة', 'الإذاعة', 'الخاصة', 'الديوان', 'أف', 'أم', 'الجائزة', 'الثانية', 'برنامج', 'القدس', 'درة', 'المدائن', 'من', 'انتاج', 'اذاعة', 'زايد', 'للقرأن', 'الكريم', 'الامارات', 'بالنسبة', 'لبرامج', 'الاطفال', 'الجائزة', 'الاولى', 'برنامج', 'كوني', 'من', 'انتاج', 'المؤسسة', 'العمومية', 'للتلفزيون', 'الجزائري', 'الجائزة', 'الثانية', 'برنامج', 'مهنتي', 'من', 'انتاج', 'تلفزيون', 'دولة', 'الكويت'], 'culture']]\n"
     ]
    }
   ],
   "source": [
    "for row in train:\n",
    "    #tokenize\n",
    "    splitted = word_tokenize(row[0])\n",
    "\n",
    "    for i in range(0, len(splitted)):\n",
    "        splitted[i] = stemmer.stem(splitted[i])\n",
    "    \n",
    "    row[0] = splitted\n",
    "print(train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('arabic')) \n",
    "for row in train:\n",
    "    x = [w for w in row[0] if not w in stop_words]\n",
    "    row[0] = \" \".join(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_sent = [line[0] for line in train]\n",
    "train_labels = [line[1] for line in train]\n",
    "\n",
    "tdif_vec = TfidfVectorizer(norm = None)\n",
    "tfidf = tdif_vec.fit_transform(train_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed the training documents and their labels to a NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file = open('test\\posts.json', encoding='utf8')\n",
    "json_reader = json.load(json_file)\n",
    "\n",
    "user_interest = []\n",
    "\n",
    "test = []\n",
    "for user in json_reader.values():\n",
    "    for article in user:\n",
    "        #use the title and description as input\n",
    "        dirty_input = article[0] + article[1]\n",
    "        \n",
    "        # lowercase and remove line breaks\n",
    "        clean_input = dirty_input.lower()\n",
    "        clean_input = dirty_input.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "        test.append(clean_input)\n",
    "\n",
    "# number of data provided for each user is 50, nubmer of users is 2000\n",
    "test = test[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL, special characters and digits removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test)):\n",
    "    test[i] = re.sub(r'http\\S+', '', test[i])\n",
    "\n",
    "    # remove special characters and digits\n",
    "    test[i] = re.sub(\"(\\\\d|\\\\W)+\",\" \", test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test)):\n",
    "    #tokenize\n",
    "    splitted = word_tokenize(test[i])\n",
    "\n",
    "    #stem\n",
    "    for j in range(0, len(splitted)):\n",
    "        splitted[j] = stemmer.stem(splitted[j])\n",
    "    \n",
    "    test[i] = splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test)):\n",
    "    x = [w for w in test[i] if not w in stop_words]\n",
    "    test[i] = \" \".join(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the labels of the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['internationalNews' 'technology' 'society' 'internationalNews' 'politic'\n",
      " 'internationalNews' 'technology' 'technology' 'localNews'\n",
      " 'internationalNews' 'internationalNews' 'diverse' 'technology' 'sport'\n",
      " 'diverse' 'sport' 'sport' 'culture' 'sport' 'politic' 'sport' 'politic'\n",
      " 'technology' 'diverse' 'sport' 'economy' 'technology' 'culture'\n",
      " 'technology' 'technology' 'internationalNews' 'diverse' 'technology'\n",
      " 'technology' 'sport' 'diverse' 'culture' 'sport' 'economy'\n",
      " 'internationalNews' 'culture' 'sport' 'culture' 'culture' 'technology'\n",
      " 'economy' 'diverse' 'sport' 'diverse' 'technology' 'economy' 'culture'\n",
      " 'technology' 'economy' 'politic' 'culture' 'economy' 'technology'\n",
      " 'technology' 'localNews' 'culture' 'sport' 'economy' 'culture'\n",
      " 'technology' 'culture' 'sport' 'diverse' 'politic' 'internationalNews'\n",
      " 'culture' 'sport' 'technology' 'economy' 'technology' 'technology'\n",
      " 'sport' 'politic' 'culture' 'culture' 'politic' 'culture' 'diverse'\n",
      " 'economy' 'sport' 'technology' 'technology' 'technology' 'technology'\n",
      " 'culture' 'internationalNews' 'technology' 'technology' 'sport' 'diverse'\n",
      " 'technology' 'culture' 'diverse' 'sport' 'culture']\n"
     ]
    }
   ],
   "source": [
    "transformed_test = tdif_vec.transform(test)\n",
    "predictions = clf.predict(transformed_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate each user interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 0: internationalNews 14.0%, technology 24.0%, society 2.0%, politic 6.0%, localNews 2.0%, diverse 14.0%, sport 20.0%, culture 12.0%, economy 8.0%,\n",
      "User 1: culture 24.0%, technology 28.0%, economy 10.0%, politic 8.0%, localNews 2.0%, sport 14.0%, diverse 8.0%, internationalNews 4.0%,\n"
     ]
    }
   ],
   "source": [
    "articles_per_user = 50\n",
    "interests = [{}]\n",
    "user_index = 0\n",
    "for i in range(0, len(predictions)):\n",
    "    if predictions[i] not in interests[user_index].keys():\n",
    "        interests[user_index][predictions[i]] = 0\n",
    "    interests[user_index][predictions[i]] = interests[user_index][predictions[i]] + 1\n",
    "    \n",
    "    if i != 0 and i % articles_per_user == 0:\n",
    "        user_index += 1\n",
    "        interests.append({})\n",
    "        \n",
    "user_index = 0\n",
    "for user in interests:\n",
    "    user_interests = ('User {}:').format(user_index)\n",
    "    for key, value in user.items():\n",
    "        user_interests = ('{} {} {}%,').format(user_interests, key, (value*100.00/articles_per_user))\n",
    "    user_index += 1\n",
    "    print(user_interests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
